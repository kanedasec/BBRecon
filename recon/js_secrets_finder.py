from __future__ import annotations

import json
import os
import re
from dataclasses import dataclass
from html import escape
from pathlib import Path
from typing import Iterable, Optional

# A pragmatic regex set inspired by SecretFinder (trimmed + maintainable)
DEFAULT_REGEX: dict[str, str] = {
    "google_api_key": r"AIza[0-9A-Za-z\-_]{35}",
    "google_oauth": r"ya29\.[0-9A-Za-z\-_]+",
    "aws_access_key": r"\bA[SK]IA[0-9A-Z]{16}\b",
    "mailgun_api_key": r"\bkey-[0-9a-zA-Z]{32}\b",
    "twilio_api_key": r"\bSK[0-9a-fA-F]{32}\b",
    "twilio_account_sid": r"\bAC[a-zA-Z0-9_\-]{32}\b",
    "stripe_live_secret": r"\bsk_live_[0-9a-zA-Z]{24}\b",
    "stripe_live_restricted": r"\brk_live_[0-9a-zA-Z]{24}\b",
    "jwt": r"eyJ[a-zA-Z0-9_\-]{10,}\.[a-zA-Z0-9_\-]{10,}\.[a-zA-Z0-9_\-]{10,}",
    "github_token_prefix": r"\bgh[pousr]_[A-Za-z0-9]{30,120}\b",
    "slack_token": r"\bxox[baprs]-[0-9A-Za-z-]{10,200}\b",
    # generic “key/value” pattern (often noisy)
    "generic_key_assign": r"(?i)\b(api[_-]?key|secret|token|bearer|authorization)\b\s*[:=]\s*['\"][^'\"]{8,}['\"]",
    # private key headers
    "private_key_header": r"-----BEGIN (?:RSA |DSA |EC |OPENSSH |PGP )?PRIVATE KEY-----",
}

HTML_TEMPLATE = """<!DOCTYPE html>
<html><head><meta charset="UTF-8">
<style>
body{font-family:system-ui,Segoe UI,Arial;margin:24px}
h2{margin:18px 0 6px}
.small{color:#555;font-size:12px}
.hit{background:#f6f6f6;border:1px solid #ddd;padding:10px;border-radius:10px;margin:8px 0}
code{background:#fff;border:1px solid #eee;padding:2px 6px;border-radius:6px}
</style>
<title>JS Secret Scan</title></head><body>
<h1>JS Secret Scan</h1>
<div class="small">Generated by recon</div>
$$content$$
</body></html>
"""

@dataclass(frozen=True)
class Hit:
    name: str
    preview: str
    line: int
    col: int

@dataclass
class FileReport:
    js_url: str
    path: str
    hits: list[Hit]

def _mask(s: str, keep_start: int = 6, keep_end: int = 4) -> str:
    s = s or ""
    if len(s) <= keep_start + keep_end:
        return s[:keep_start] + "*" * max(0, len(s) - keep_start)
    return s[:keep_start] + "*" * (len(s) - keep_start - keep_end) + s[-keep_end:]

def _line_col(text: str, idx: int) -> tuple[int, int]:
    # 1-based line/col
    line = text.count("\n", 0, max(0, idx)) + 1
    last_nl = text.rfind("\n", 0, max(0, idx))
    col = idx + 1 if last_nl == -1 else (idx - last_nl)
    return line, col

def compile_regex(regex_map: dict[str, str]) -> dict[str, re.Pattern]:
    compiled: dict[str, re.Pattern] = {}
    for name, pat in regex_map.items():
        compiled[name] = re.compile(pat, re.IGNORECASE | re.MULTILINE)
    return compiled

def scan_text(text: str, compiled: dict[str, re.Pattern], max_hits_per_file: int = 200) -> list[Hit]:
    hits: list[Hit] = []
    seen = set()
    for name, rx in compiled.items():
        for m in rx.finditer(text):
            val = m.group(0)
            if not val:
                continue
            # de-dupe within file by (name + value)
            key = (name, val)
            if key in seen:
                continue
            seen.add(key)

            line, col = _line_col(text, m.start())
            hits.append(Hit(name=name, preview=val, line=line, col=col))

            if len(hits) >= max_hits_per_file:
                return hits
    # stable ordering: by line then name
    hits.sort(key=lambda h: (h.line, h.name))
    return hits

def scan_files(
    items: Iterable[tuple[str, str]],
    regex_map: dict[str, str] | None = None,
    max_bytes: int = 5_000_000,
    max_hits_per_file: int = 200,
) -> list[FileReport]:
    regex_map = regex_map or DEFAULT_REGEX
    compiled = compile_regex(regex_map)

    reports: list[FileReport] = []
    for js_url, path in items:
        p = Path(path).expanduser()
        if not p.exists() or not p.is_file():
            continue

        # avoid reading huge files
        if p.stat().st_size > max_bytes:
            continue

        try:
            raw = p.read_bytes()
            text = raw.decode("utf-8", errors="ignore")
        except Exception:
            continue

        hits = scan_text(text, compiled, max_hits_per_file=max_hits_per_file)
        if hits:
            reports.append(FileReport(js_url=js_url, path=str(p), hits=hits))

    return reports

def write_json_report(reports: list[FileReport], out_path: str) -> str:
    Path(out_path).expanduser().resolve().parent.mkdir(parents=True, exist_ok=True)
    payload = []
    for r in reports:
        payload.append({
            "js_url": r.js_url,
            "path": r.path,
            "hits": [{"name": h.name, "preview": h.preview, "line": h.line, "col": h.col} for h in r.hits],
        })
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
    return out_path

def write_html_report(reports: list[FileReport], out_path: str) -> str:
    Path(out_path).expanduser().resolve().parent.mkdir(parents=True, exist_ok=True)

    parts = []
    for r in reports:
        parts.append(f"<h2>{escape(r.js_url)}</h2>")
        parts.append(f"<div class='small'>file: <code>{escape(r.path)}</code></div>")
        for h in r.hits:
            parts.append(
                "<div class='hit'>"
                f"<div><b>{escape(h.name)}</b> — line {h.line}, col {h.col}</div>"
                f"<div><code>{escape(h.preview)}</code></div>"
                "</div>"
            )

    html = HTML_TEMPLATE.replace("$$content$$", "\n".join(parts))
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(html)
    return out_path
